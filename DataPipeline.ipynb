{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f53d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:35.163330Z",
     "start_time": "2023-05-04T22:37:31.557848Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a090413a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:35.178624Z",
     "start_time": "2023-05-04T22:37:35.165353Z"
    }
   },
   "outputs": [],
   "source": [
    "max_snt_len = 256\n",
    "models = [\n",
    "    '''michellejieli/emotion_text_classifier''',\n",
    "    '''microsoft/deberta-v3-base''',\n",
    "    '''sileod/deberta-v3-base-tasksource-nli''',\n",
    "    '''microsoft/deberta-v3-xsmall'''\n",
    "]\n",
    "model_id = 2\n",
    "model_name = models[model_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc935dd",
   "metadata": {},
   "source": [
    "**Emocause dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42aa4b9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:35.219362Z",
     "start_time": "2023-05-04T22:37:35.179650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original_situation': 'I felt betrayed when my girlfriend kissed another guy at a party. She was drunk, true. But still.', 'tokenized_situation': ['I', 'felt', 'betrayed', 'when', 'my', 'girlfriend', 'kissed', 'another', 'guy', 'at', 'a', 'party', '.', 'She', 'was', 'drunk', ',', 'true', '.', 'But', 'still', '.'], 'emotion': '__disappointed__', 'conv_id': 'hit:4449_conv:8898', 'annotation': [['girlfriend', 5], ['kissed', 6], ['another', 7], ['guy', 8]], 'labels': ['girlfriend', 'kissed', 'another', 'guy']}\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/emocause/valid.json\", 'r') as f:\n",
    "    emocause_data_valid = json.load(f)\n",
    "with open(\"data/emocause/test.json\", 'r') as f:\n",
    "    emocause_data_test = json.load(f)\n",
    "\n",
    "print(emocause_data_valid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8caca59b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:35.235453Z",
     "start_time": "2023-05-04T22:37:35.219362Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_emocause_dataset(data, prefix=False):\n",
    "    offset = 2 if prefix else 0\n",
    "    dataset = []\n",
    "    for obj in data:\n",
    "        if len(obj['tokenized_situation']) >= max_snt_len - offset:\n",
    "            continue\n",
    "        x = [obj['emotion'][2].upper() + obj['emotion'][3:-2], '. '] if prefix else []\n",
    "        x.extend(obj['tokenized_situation'])\n",
    "        y = np.zeros(len(x), dtype=int)\n",
    "        for word in obj['annotation']:\n",
    "            y[word[1] + offset] = 1\n",
    "        dataset.append({'text': x, 'labels': y, 'count': [len(obj['annotation'])]})\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7f54c5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:35.337569Z",
     "start_time": "2023-05-04T22:37:35.236445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I', 'miss', 'being', 'in', 'high', 'school', '.', 'I', 'still', 'remember', 'my', 'old', 'girlfriend', 'fondly'], 'labels': [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0], 'count': [4]}\n"
     ]
    }
   ],
   "source": [
    "emocause_train, emocause_valid = train_test_split(emocause_data_valid, test_size=0.2, random_state=42)\n",
    "emocause_train_ds = prepare_emocause_dataset(emocause_train)\n",
    "emocause_val_ds = prepare_emocause_dataset(emocause_valid)\n",
    "emocause_test_ds = prepare_emocause_dataset(emocause_data_test)\n",
    "\n",
    "emocause_hg_train = datasets.Dataset.from_list(emocause_train_ds)\n",
    "emocause_hg_val = datasets.Dataset.from_list(emocause_val_ds)\n",
    "emocause_hg_test = datasets.Dataset.from_list(emocause_test_ds)\n",
    "print(emocause_hg_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a6249a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:46.197150Z",
     "start_time": "2023-05-04T22:37:35.337569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ce5f2cf1c54f749db77cbedcd64b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\miniconda3\\envs\\torch\\lib\\site-packages\\huggingface_hub-0.13.2-py3.8.egg\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45274c4216b9494297351e176c664a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6b52864dc44cefafb7717266b66639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/8.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ede86aff23745db9086bb24393f449a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81d1e723b4c49baa9bcd8678312723f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3020 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels', 'count', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 3020\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], max_length=max_snt_len, \n",
    "                     truncation=True, padding=\"max_length\", is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "emocause_train_dataset = emocause_hg_train.map(tokenize_and_align_labels, batched=True)\n",
    "emocause_val_dataset = emocause_hg_val.map(tokenize_and_align_labels, batched=True)\n",
    "emocause_test_dataset = emocause_hg_test.map(tokenize_and_align_labels, batched=True)\n",
    "print(emocause_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "861df254",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:46.250103Z",
     "start_time": "2023-05-04T22:37:46.199144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([-100,    0,    0,    0,    0,    1,    1,    0,    0,    0,    0,    0,\n",
      "           1,    1,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100]), 'count': tensor([4]), 'input_ids': tensor([    1,   273,  2374,   411,   267,   459,   563,   323,   273,   449,\n",
      "         1221,   312,   597,  7334, 32547,     2,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "emocause_train_dataset = emocause_train_dataset.remove_columns(['text'])\n",
    "emocause_val_dataset = emocause_val_dataset.remove_columns(['text'])\n",
    "emocause_test_dataset = emocause_test_dataset.remove_columns(['text'])\n",
    "emocause_train_dataset.set_format(\"torch\")\n",
    "emocause_val_dataset.set_format(\"torch\")\n",
    "emocause_test_dataset.set_format(\"torch\")\n",
    "print(emocause_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6990541",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:46.265579Z",
     "start_time": "2023-05-04T22:37:46.250103Z"
    }
   },
   "outputs": [],
   "source": [
    "data = (emocause_train_dataset, emocause_val_dataset, emocause_test_dataset)\n",
    "with open('./data/emocause/data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9b9ec",
   "metadata": {},
   "source": [
    "**EmpatheticDialogues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59a278c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:46.280923Z",
     "start_time": "2023-05-04T22:37:46.266600Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_empdia_dataset(path):\n",
    "    ds = pd.read_csv(path, on_bad_lines='skip')\n",
    "    ds2 = ds.loc[ds.utterance_idx == 1]\n",
    "    ds2.drop(columns=['conv_id', 'utterance_idx', 'speaker_idx', 'utterance', 'selfeval', 'tags'], inplace=True)\n",
    "    ds2['prompt'] = ds2['prompt'].apply(lambda x: x.replace(\"_comma_\", \",\"))\n",
    "    return ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efda0661",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:46.798159Z",
     "start_time": "2023-05-04T22:37:46.283920Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2832\\1078292434.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds2.drop(columns=['conv_id', 'utterance_idx', 'speaker_idx', 'utterance', 'selfeval', 'tags'], inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2832\\1078292434.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds2['prompt'] = ds2['prompt'].apply(lambda x: x.replace(\"_comma_\", \",\"))\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2832\\1078292434.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds2.drop(columns=['conv_id', 'utterance_idx', 'speaker_idx', 'utterance', 'selfeval', 'tags'], inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2832\\1078292434.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds2['prompt'] = ds2['prompt'].apply(lambda x: x.replace(\"_comma_\", \",\"))\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2832\\1078292434.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds2.drop(columns=['conv_id', 'utterance_idx', 'speaker_idx', 'utterance', 'selfeval', 'tags'], inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2832\\1078292434.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds2['prompt'] = ds2['prompt'].apply(lambda x: x.replace(\"_comma_\", \",\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>afraid</td>\n",
       "      <td>i used to scare for darkness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>proud</td>\n",
       "      <td>I showed a guy how to run a good bead in weldi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>faithful</td>\n",
       "      <td>I have always been loyal to my wife.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>terrified</td>\n",
       "      <td>A recent job interview that I had made me feel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        context                                             prompt\n",
       "0   sentimental  I remember going to the fireworks with my best...\n",
       "6        afraid                       i used to scare for darkness\n",
       "12        proud  I showed a guy how to run a good bead in weldi...\n",
       "17     faithful               I have always been loyal to my wife.\n",
       "21    terrified  A recent job interview that I had made me feel..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_pd = prepare_empdia_dataset(\"./data/empatheticdialogues/train.csv\")\n",
    "val_data_pd = prepare_empdia_dataset(\"./data/empatheticdialogues/valid.csv\")\n",
    "test_data_pd = prepare_empdia_dataset(\"./data/empatheticdialogues/test.csv\")\n",
    "\n",
    "train_data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3525a3d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:46.827545Z",
     "start_time": "2023-05-04T22:37:46.798159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'prompt', '__index_level_0__'],\n",
      "    num_rows: 17797\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "labels = train_data_pd.context.unique()\n",
    "id2label = dict(zip(range(len(labels)), labels))\n",
    "label2id = dict(zip(labels, range(len(labels))))\n",
    "train_data_pd['context'] = train_data_pd['context'].apply(lambda x: label2id[x])\n",
    "val_data_pd['context'] = val_data_pd['context'].apply(lambda x: label2id[x])\n",
    "test_data_pd['context'] = test_data_pd['context'].apply(lambda x: label2id[x])\n",
    "\n",
    "hg_train_data = Dataset.from_pandas(train_data_pd)\n",
    "hg_test_data = Dataset.from_pandas(test_data_pd)\n",
    "hg_val_data = Dataset.from_pandas(val_data_pd)\n",
    "\n",
    "print(hg_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62dde332",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:52.262008Z",
     "start_time": "2023-05-04T22:37:46.828546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2761 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def tokenize_dataset(examples):\n",
    "    return tokenizer(examples['prompt'], \n",
    "                     max_length=max_snt_len, \n",
    "                     truncation=True, \n",
    "                     padding=\"max_length\")\n",
    "train_dataset = hg_train_data.map(tokenize_dataset)\n",
    "test_dataset = hg_test_data.map(tokenize_dataset)\n",
    "val_dataset = hg_val_data.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0eec0ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:52.293591Z",
     "start_time": "2023-05-04T22:37:52.262008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor(0), 'input_ids': tensor([    1,   273,  1221,   446,   264,   262, 13111,   275,   312,   410,\n",
      "         1156,   260,   443,   284,   266,   509,   265,   355,   261,   304,\n",
      "          278,   364,  1199,   334,   381,   267,   262,   447,   260,     2,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.remove_columns(['__index_level_0__', 'prompt'])\n",
    "test_dataset = test_dataset.remove_columns(['__index_level_0__', 'prompt'])\n",
    "val_dataset = val_dataset.remove_columns(['__index_level_0__', 'prompt'])\n",
    "train_dataset = train_dataset.rename_column(\"context\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"context\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"context\", \"labels\")\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "val_dataset.set_format(\"torch\")\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c8381eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T22:37:52.390284Z",
     "start_time": "2023-05-04T22:37:52.295561Z"
    }
   },
   "outputs": [],
   "source": [
    "data2 = (train_dataset, val_dataset, test_dataset)\n",
    "with open('./data/empatheticdialogues/data.pickle', 'wb') as f:\n",
    "    pickle.dump(data2, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
